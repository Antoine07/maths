---
marp: true
theme: default
paginate: true
class: lead
---

## **Matrices, vecteurs, transformations et vecteurs propres**

---

## Introduction 

D'o√π viennent les √©quations ou matrices

## **Dimension 1 : d√©croissance simple**

On mod√©lise une quantit√© qui diminue de 20 % √† chaque p√©riode
(population, prix, signal‚Ä¶).

Relation r√©elle :

$$
x_{\text{nouveau}} = 0.8 , x_{\text{ancien}}
$$

---

Ici :

1. le coefficient **0.8** exprime une perte de 20 %,
1. la ¬´ transformation ¬ª est simplement **une matrice 1√ó1** :
  $$
  [0.8]
  $$

Ce mod√®le dit juste : *chaque p√©riode, on garde 80 % de la valeur pr√©c√©dente*.

---

# Dimension 2 : industrie et services

Deux secteurs se stimulent mutuellement :

1. l'industrie g√©n√®re +0.7 de nouvelle industrie et +0.1 de services,
1. les services g√©n√®rent +0.2 d'industrie et +0.8 de services.

Alors :

$$
\begin{pmatrix}
\text{Ind}*{n+1} \\
\text{Serv}*{n+1}
\end{pmatrix}
=

\begin{pmatrix}
0.7 & 0.1 \\
0.2 & 0.8
\end{pmatrix}
\begin{pmatrix}
\text{Ind}_n \\
\text{Serv}_n
\end{pmatrix}
$$

La matrice traduit simplement :

1. comment **chaque secteur influence l'autre**,
1. et comment chacun **s'auto-alimente**.

---

## I. Vecteurs et matrices

### 1. Vecteurs

Un vecteur est un ensemble ordonn√© de nombres.

$$
x =
\begin{pmatrix}
2 \\
-1
\end{pmatrix}
$$

Il repr√©sente une direction et une intensit√© dans l'espace.

---

### 2. Matrices

Une matrice est un tableau de nombres.

$$
A=
\begin{pmatrix}
1 & 3 \\
2 & -1
\end{pmatrix}
$$

Elle repr√©sente une transformation lin√©aire : elle prend un vecteur en entr√©e et produit un nouveau vecteur.

---

## II.1 Multiplication matrice‚Äìvecteur

Pour une matrice (A) et un vecteur (x), le produit (Ax) est d√©fini par :

$$
Ax=
\begin{pmatrix}
a_{11}x_1 + a_{12}x_2 \\
a_{21}x_1 + a_{22}x_2
\end{pmatrix}
$$

Exemple :

$$
A=
\begin{pmatrix}
1 & 2 \\
3 & 1
\end{pmatrix},
\quad
x=
\begin{pmatrix}
2\\
1
\end{pmatrix}
$$

d'o√π :

$$
Ax=
\begin{pmatrix}
1\cdot 2 + 2\cdot 1 \\
3\cdot 2 + 1\cdot 1
\end{pmatrix}
=
\begin{pmatrix}
4 \\
7
\end{pmatrix}
$$

Interpr√©tation : la matrice transforme le vecteur en l'√©tirant, le tournant ou le comprimant.

---

## II.2 Multiplication matrice‚Äìmartice

Voici la **r√®gle** pour multiplier deux matrices **2√ó2** :

$$
\begin{pmatrix}
a & b \\
c&d
\end{pmatrix}

\begin{pmatrix}
e & f\\
g & h
\end{pmatrix}
=

\begin{pmatrix}
ae + bg & af + bh \\
ce + dg & cf + dh
\end{pmatrix}.
$$

üëâ **Ligne √ó Colonne** pour chaque √©l√©ment.

---
Voici la **r√®gle tr√®s** pour multiplier deux matrices **3√ó3** :

$$
C = A \times B,\quad C_{ij} = \text{somme des produits }(\text{ligne } i \text{ de } A)\times(\text{colonne } j \text{ de } B).
$$

Autrement dit :

> **Chaque √©l√©ment = ligne de A √ó colonne de B.**


---

Pour multiplier :

$$
A_{n \times p} \quad \text{et} \quad B_{p \times m},
$$

il faut :

$$
\boxed{\text{colonnes de A} = \text{lignes de B}}.
$$

Donc **p = p**


---

**Magie de Numpy**

Pour multiplier deux matrices il suffit d'utiliser l'op√©rateur `@` comme suit dans Numpy

`C = A @ B`

A et B sont deux matrices Numpy `np.array`, l'op√©rateur ne marche que sur les array `numpy`

---

**Qu'est ce que ces dimensions peuvent repr√©senter ?**

n = le nombre d'individus / situations / observations

p = le nombre de variables / param√®tres / donn√©es par individu

---

## III. Transformations lin√©aires

Une matrice carr√©e repr√©sente une transformation lin√©aire de l'espace vers lui-m√™me.

Voici les transformations fondamentales.

### 1. √âtirement

$$
A=
\begin{pmatrix}
3 & 0 \\
0 & 1
\end{pmatrix}
$$

Multiplie la coordonn√©e (x) par 3, laisse (y) inchang√©.

---

### 2. Compression

$$
A=
\begin{pmatrix}
0.5 & 0\\
0 & 0.5
\end{pmatrix}
$$

R√©duit toutes les directions d'un facteur 2.

---

### 3. R√©flexion (sym√©trie)

$$
A=
\begin{pmatrix}
1 & 0\\
0 & -1
\end{pmatrix}
$$

Sym√©trie par rapport √† l'axe (x).

---

### 4. Rotation

$$
A=
\begin{pmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{pmatrix}
$$

Rotation de tout l'espace d'un angle (\theta).

---

## IV. Matrices particuli√®res : matrices sym√©triques

Une matrice est sym√©trique si :

$$
A = A^T,
$$
c'est-√†-dire :
$$
a_{ij} = a_{ji}
$$

Exemple :

$$
S=
\begin{pmatrix}
2 & 1\\
1 & 3
\end{pmatrix}
$$

Les matrices sym√©triques poss√®dent des propri√©t√©s g√©om√©triques fortes, notamment l'existence de vecteurs propres orthogonaux.

---

## V. Vecteurs propres et valeurs propres

### 1. D√©finition

Pour une matrice carr√©e (A), un vecteur non nul (v) est un vecteur propre s'il existe un r√©el `lambda` tel que :

$$
A v = \lambda v
$$

1. (v) est une direction pr√©serv√©e,


$$
\lambda
$$ 

est le facteur d'√©tirement dans cette direction.

---

### 2. Exemple

$$
A=
\begin{pmatrix}
3 & 0\\
0 & 2
\end{pmatrix}.
$$

Alors :

$$
A
\begin{pmatrix}
1\\
0
\end{pmatrix}
=
\begin{pmatrix}
3\\
0
\end{pmatrix}
= 3
\begin{pmatrix}
1\\
0
\end{pmatrix}
$$

Donc `(1,0)^T` est un vecteur propre, valeur propre `lambda = 3`. 

De m√™me :

$$
A
\begin{pmatrix}
0\\
1
\end{pmatrix}
=
2
\begin{pmatrix}
0\\
1
\end{pmatrix}
$$

---

### 3. Interpr√©tation g√©om√©trique

Une matrice poss√®de g√©n√©ralement plusieurs directions privil√©gi√©es : ses vecteurs propres.
Ce sont les directions o√π la transformation agit de mani√®re la plus simple.

Si l'on observe deux donn√©es comme la taille et le poids, les vecteurs propres indiquent les directions dans lesquelles ces donn√©es varient le plus ensemble. Par exemple, on peut d√©couvrir qu'une augmentation de taille s'accompagne souvent d'une augmentation de poids : cette relation correspond √† une direction propre. Les vecteurs propres servent donc √† rep√©rer les tendances naturelles dans les donn√©es.

---

Cette notion sont essentielles dans:

1. analyser les directions principales d'une fonction (via une Hessienne),
1. comprendre des donn√©es (ACP),
1. √©tudier la stabilit√© de syst√®mes,
1. optimiser des crit√®res.

---

#  Comment obtenir

$$
A = P D P^{-1}
$$

La diagonalisation consiste √† **r√©√©crire la matrice A dans une base form√©e de ses vecteurs propres**.

Voici les √©tapes exactes.

---

# 1. Trouver les valeurs propres

On r√©sout :

$$
\det(A - \lambda I) = 0
$$

Les solutions 

$$
\lambda_1, \lambda_2, \dots
$$ 

sont les **valeurs propres**.

---

# 2. Trouver les vecteurs propres

Pour chaque valeur propre  `lambda`, on r√©sout :

$$
(A - \lambda I)v = 0.
$$

Les solutions non nulles sont les **vecteurs propres**.

---

# 3. Construire la matrice (P)

On met **chaque vecteur propre comme colonne** :

$$
P = [v_1 ; v_2 ; \dots ; v_n].
$$

Si la matrice est diagonalisable, (P) est **inversible**.

---

# 4. Construire la matrice diagonale (D)

On place les valeurs propres **dans le m√™me ordre que les vecteurs propres de (P)** :

$$
D =
\begin{pmatrix}
\lambda_1 & 0 & \cdots \\
0 & \lambda_2 & \cdots \\
\vdots & & \ddots
\end{pmatrix}.
$$

---

# 5. Alors on obtient automatiquement :

$$
A P = P D
$$

Pourquoi ?

Parce que :

$$
A v_i = \lambda_i v_i
\quad\text{pour chaque vecteur propre } v_i.
$$

√âcrit sous forme matricielle :

$$
A[v_1 ; v_2 ;\dots; v_n]
=

[v_1 ; v_2 ;\dots; v_n]
\begin{pmatrix}
\lambda_1 & 0 & \dots \\
0 & \lambda_2 & \dots \\
\vdots & & \ddots
\end{pmatrix}.
$$

---

Donc :

$$
A P = P D.
$$

Et comme (P) est inversible :

$$
A = P D P^{-1}
$$

---

# R√©sum√©

> On diagonalise une matrice en construisant la base de ses vecteurs propres : dans cette base, la matrice devient diagonale, ce qui donne


$$
A = P D P^{-1}
$$


---

### 4. Th√©or√®mes importants

**Matrices sym√©triques r√©elles.**
Toute matrice sym√©trique r√©elle admet une famille compl√®te de vecteurs propres r√©els, mutuellement orthogonaux, qui forment une base de l'espace.

**Matrices diagonalisables.**
Toute matrice diagonalisable poss√®de suffisamment de vecteurs propres pour former une base.
Ces vecteurs propres ne sont pas n√©cessairement orthogonaux.

**Autres matrices.**
Une matrice qui n'est pas diagonalisable ne poss√®de pas assez de vecteurs propres pour former une base (et peut m√™me ne pas en poss√©der du tout).


---

## VI. Combinaison lin√©aire et base de vecteurs propres

De nombreuses matrices, notamment les matrices sym√©triques, peuvent √™tre diagonaliseÃÅes :

$$
A = P D P^{-1},
$$

o√π (D) est diagonale, form√©e des valeurs propres.

Cette id√©e servira plus tard pour :

1. lire facilement la structure d'une transformation,
2. identifier les directions principales d'un probl√®me.

Diagonaliser, c'est trouver les axes naturels du probl√®me.

---

## Remarque en statistiques

La covariance mesure comment deux variables varient ensemble : positive si elles montent ou descendent ensemble, n√©gative si l'une monte quand l'autre descend.

La matrice de covariance est diagonalis√©e :
‚Üí Les vecteurs propres = axes principaux
‚Üí Les valeurs propres = variances importantes


> **(A) et (D) font la m√™me chose : (A) le fait dans la base normale, (D) le fait dans la base des vecteurs propres o√π tout est plus simple.**

---


# Inverse d'une matrice : m√©thode de la matrice augment√©e

Une matrice (A) est inversible s'il existe `InvA` tel que :

$$
A A^{-1} = I.
$$

Pour trouver `InvA`, on utilise **la matrice augment√©e** :

$$
(A \mid I),
$$

et on applique des op√©rations √©l√©mentaires pour transformer :

$$
(A \mid I) ;\longrightarrow; (I \mid A^{-1}).
$$

---

## Exemple 

Soit

$$
A=
\begin{pmatrix}
2 & 1\\
1 & 1
\end{pmatrix}.
$$

On forme :

$$
\left(
\begin{array}{cc|cc}
2 & 1 & 1 & 0\\
1 & 1 & 0 & 1
\end{array}
\right).
$$

√âtape 1 : rendre le pivot en haut √† gauche √©gal √† 1 (on √©change les lignes) :

$$
\left(
\begin{array}{cc|cc}
1 & 1 & 0 & 1\\
2 & 1 & 1 & 0
\end{array}
\right).
$$

---

√âtape 2 : annuler la deuxi√®me ligne, premi√®re colonne :

L2 ‚Üê L2 ‚Äì 2 L1

$$
\left(
\begin{array}{cc|cc}
1 & 1 & 0 & 1\\
0 & -1 & 1 & -2
\end{array}
\right).
$$

---

√âtape 3 : normaliser la deuxi√®me ligne

L2 ‚Üê ‚ÄìL2

$$
\left(
\begin{array}{cc|cc}
1 & 1 & 0 & 1\\
0 & 1 & -1 & 2
\end{array}
\right).
$$

---

√âtape 4 : annuler le 1 au-dessus (colonne 2)

L1 ‚Üê L1 ‚Äì L2

$$
\left(
\begin{array}{cc|cc}
1 & 0 & 1 & -1\\
0 & 1 & -1 & 2
\end{array}
\right).
$$

R√©sultat :

$$
A^{-1}=
\begin{pmatrix}
1 & -1\\
-1 & 2
\end{pmatrix}.
$$

---

# V√©rification

$$
A A^{-1} =
\begin{pmatrix}
2 & 1\\
1 & 1
\end{pmatrix}
\begin{pmatrix}
1 & -1\\
-1 & 2
\end{pmatrix}
=

\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}.
$$

L'inverse est correctement trouv√©.

---

# R√©solution d'un syst√®me lin√©aire `2x2`

Un syst√®me de deux √©quations √† deux inconnues s'√©crit :

$$
\begin{cases}
a_{11} x + a_{12} y = b_1 \\
a_{21} x + a_{22} y = b_2
\end{cases}
$$

---

# Forme matricielle

$$
A x = b,
\quad
A =
\begin{pmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{pmatrix},
\quad
x =
\begin{pmatrix}
x \\ y
\end{pmatrix},
\quad
b =
\begin{pmatrix}
b_1 \\ b_2
\end{pmatrix}.
$$

Le but est de trouver le vecteur `x`

---

# Condition d'existence d'une solution unique

Une unique solution existe **si et seulement si** :

$$
\det(A) \neq 0.
$$

Pour une matrice 2√ó2 :

$$
\det(A) = a_{11} a_{22} - a_{12} a_{21}.
$$

---


## Exercice 1

R√©soudre √† l'aide de la matrice augment√©e, puis v√©rifiez √† l'aide `np.linalg.inv(A)` avec Numpy que vous avez bien la matrice invers√©e.

$$
\begin{cases}
3x + 2y = 7 \\
x - y = 1
\end{cases}
$$

---

## Exercice 2
R√©soudre par la matrice augment√©e :

$$
\begin{cases}
4x - y = 10 \\
2x + y = 8
\end{cases}
$$

---

## Exercice 3
D√©terminer si le syst√®me poss√®de une solution unique :

$$
\begin{cases}
2x + 4y = 8 \\
x + 2y = 4
\end{cases}
$$

---

# R√©solution d'un syst√®me lin√©aire `3x3` avec NumPy

Un syst√®me lin√©aire s'√©crit sous forme matricielle :

$$
A x = b
$$

o√π `A` est une matrice 3x3, `x` le vecteur des inconnues, et `b` le vecteur des r√©sultats.

NumPy permet de r√©soudre directement ce type de syst√®me.

---

## M√©thode

Pour une unique solution, il faut que :

$$
\det(A) \neq 0.
$$

NumPy v√©rifie cette condition automatiquement.

La r√©solution se fait avec :

```python
x = np.linalg.solve(A, b)
```

---

## Exemple

R√©soudre :

$$
\begin{cases}
x + y + z = 6\\
2x - y + 3z = 14\\
-x + 4y + z = 2
\end{cases}
$$

---

```python
import numpy as np

A = np.array([
    [1, 1, 1],
    [2, -1, 3],
    [-1, 4, 1]
])

b = np.array([6, 14, 2])

x = np.linalg.solve(A, b)
print(x)
```

R√©sultat :

$$
x=3,\quad y=1,\quad z=2.
$$

---

## V√©rification

```python
A @ x    # doit √™tre √©gal √† b
```

---

## Remarque

> Audelas de la dimension 3x3 on utilise d'autres m√©thodes.
> Pour des syst√®mes de taille sup√©rieure √† `3x3` on utilise des m√©thodes num√©riques robustes comme la d√©composition LU ou QR. 
> NumPy applique automatiquement ces m√©thodes via `np.linalg.solve`

---

# M√©thode stable

```python
import numpy as np

A = np.array([
    [2, 1, 0, 3],
    [1, 4, 2, 1],
    [0, 2, 5, 2],
    [3, 1, 2, 6]
], dtype=float)

b = np.array([7, 12, 15, 20], dtype=float)

x = np.linalg.solve(A, b)
print("Solution :", x)

```

Si √ßa √©choue : `LinAlgError: Singular matrix`

---

## Application - Trouvez les vecteurs propres

Utilisez Python pour trouver les vecteurs propres de la matrice suivante:

$$
A=
\begin{pmatrix}
4 & 1 & 2\\
0 & 3 & 5\\
1 & 0 & 2\\
\end{pmatrix}.
$$

---

Testez le programme suivant

```python
import numpy as np

A = np.array([
    [4, 1, 2],
    [0, 3, 5],
    [1, 0, 2]
])

# Calcul des valeurs propres et vecteurs propres
valeurs_propres, vecteurs_propres = np.linalg.eig(A)
print(valeurs_propres)
print(vecteurs_propres)
```

---

## Exercice **Interaction entre deux types de cellules**

Deux types de cellules interagissent :

1. (x) : cellules immunitaires
1. (y) : cellules tumorales

On mod√©lise leurs influences respectives par :

$$
A=
\begin{pmatrix}
4 & 2\\
2 & 4
\end{pmatrix}.
$$

Cette matrice d√©crit comment les deux populations agissent l'une sur l'autre dans un mod√®le simplifi√©.

---
Sans utilisez Python r√©pondez aux questions suivantes:

1. Calculez les valeurs propres de (A).
2. Trouvez les vecteurs propres associ√©s.
3. Construisez les matrices (P) et (D).
4. Interpr√©tez biologiquement les deux vecteurs propres.


V√©rifiez l'ensemble de vos calcules √† l'aide de Python.

---

##  **Interpr√©tation biologique**

$$
(1,1)
\quad\text{avec}\quad
\lambda_1 = 6
$$

Cela d√©crit **une √©volution conjointe** o√π les deux populations cellulaires augmentent ou diminuent *ensemble*.

‚Üí **Comportement dominant**, coop√©ration forte.

---

###  Deuxi√®me vecteur propre

$$
(1,-1)
\quad\text{avec}\quad
\lambda_2 = 2
$$

Cela d√©crit une **relation oppos√©e** :

1. si les cellules immunitaires augmentent,
1. les cellules tumorales diminuent (et inversement).

‚Üí **Comportement secondaire**, oppos√© des dynamiques.

---

# TP dynamique √©conomique

[Dynamique des esp√®ces](https://github.com/Antoine07/maths/blob/main/Matrice/TPs/TP_dynamique_especes.md)

---

## [Retour au plan](https://antoine07.github.io/maths/)